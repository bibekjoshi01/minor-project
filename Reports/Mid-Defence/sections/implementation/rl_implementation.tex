\subsection{Reinforcement Learning Implementation}

This section describes the practical implementation of the reinforcement learning (RL) based antenna alignment system. The focus is on system integration, data structures, and execution flow.

\subsubsection{State, Action, and Reward Representation}

\textbf{State Definition:} The reinforcement learning agent operates on a discrete state representation derived from antenna orientation and recent signal behavior. Each state is defined as a tuple:

\begin{equation}
s_k = \left( i_{\text{pan}},\; i_{\text{tilt}},\; \text{trend}_k \right)
\end{equation}

where:
\begin{itemize}
    \item $i_{\text{pan}}$ is the discretized pan angle index,
    \item $i_{\text{tilt}}$ is the discretized tilt angle index,
    \item $\text{trend}_k \in \{-1, 0, +1\}$ represents RSSI decrease, stability, or increase.
\end{itemize}

This compact representation enables the agent to reason about both spatial alignment and short-term signal dynamics while keeping the state space tractable.

\textbf{Action Space:} The action space consists of small, discrete pan and tilt adjustments that can be executed directly by the stepper motors. Each action corresponds to a fixed angular movement:

\begin{equation}
\mathcal{A} = \{
(+\Delta_p, 0), (-\Delta_p, 0),
(0, +\Delta_t), (0, -\Delta_t),
(0, 0)
\}
\end{equation}

where $\Delta_p$ and $\Delta_t$ are predefined step sizes for pan and tilt respectively.  
The \textit{no-movement} action allows the agent to maintain its current orientation when further movement is unnecessary.

\textbf{Reward Shaping:} The reward function is designed to encourage RSSI improvement while discouraging excessive or unnecessary movement.

\begin{equation}
r_k = \Delta \text{RSSI}_k - \lambda \cdot \mathbb{I}_{\text{move}}
\end{equation}

where:
\begin{itemize}
    \item $\Delta \text{RSSI}_k = \overline{\text{RSSI}}_k - \overline{\text{RSSI}}_{k-1}$,
    \item $\mathbb{I}_{\text{move}}$ is an indicator function equal to 1 if a motor movement occurs and 0 otherwise,
    \item $\lambda$ is a small movement penalty coefficient.
\end{itemize}

This formulation biases the policy toward faster convergence and reduces mechanical wear by avoiding unnecessary actuator motion.


\subsubsection{Q-Table Storage and Deployment}

\textbf{Offline Training:} All reinforcement learning training is performed offline on a personal computer using a simulated antenna environment. The simulation incorporates RSSI noise, side lobes, and realistic movement delays to approximate hardware behavior.

\textbf{Serialization:} After training, the learned Q-table is serialized and stored using a binary format:

\begin{itemize}
    \item \texttt{.npy} format for direct NumPy compatibility
    \item Conversion to C-style arrays for firmware integration
\end{itemize}

\textbf{Firmware Deployment:} During deployment, the trained Q-table is embedded into the ESP32-S3 firmware as a static data structure. The microcontroller executes a fixed greedy policy:

\begin{equation}
a^* = \arg\max_a Q(s, a)
\end{equation}

No Q-value updates are performed on the device.

\textbf{Design Decision:} Online learning is intentionally disabled on the embedded system for the following reasons:
\begin{itemize}
    \item Ensures deterministic and stable behavior
    \item Avoids divergence due to noisy RSSI measurements
    \item Reduces computational and memory overhead on the ESP32
\end{itemize}

As a result, the deployed system behaves as a rule-based controller derived from offline reinforcement learning.

% ---------------------------------------------------

\subsubsection{Control Flow of RL-Based Alignment}

\textbf{Initialization:} Upon system startup or manual reset, the antenna performs a coarse exhaustive scan to obtain an initial alignment close to the global RSSI maximum. This reduces the dependency of the RL agent on its starting state.

\begin{figure}[H]
    \vspace{1em} 
    \centering
    \includegraphics[width=1\textwidth]{images/flowdiagram.png}
    \caption{Control flow of the RL-based antenna alignment system}
    \label{fig:rl_flowchart}
\end{figure}

\textbf{RL-Based Fine Alignment:} After initialization, the system enters the reinforcement learning control loop:
\begin{itemize}
    \item The current state is encoded from antenna orientation and RSSI trend
    \item An action is selected using a greedy policy from the Q-table
    \item The antenna orientation is updated
    \item RSSI is measured and the new state is computed
\end{itemize}

\textbf{Convergence Handling:} The alignment process is considered converged when:
\begin{itemize}
    \item RSSI improvement remains below a threshold for a predefined number of steps, or
    \item The maximum number of RL steps is reached
\end{itemize}

Once converged, the antenna holds its position until a trigger event occurs.

\textbf{Re-Alignment Triggers:} The RL loop may be reactivated under the following conditions:
\begin{itemize}
    \item Significant RSSI degradation beyond a threshold
    \item Manual user reset
    \item Periodic re-evaluation in dynamic environments
\end{itemize}

The complete execution sequence is summarized in Figure~\ref{fig:rl_flowchart}, which depicts the transition from initialization to RL-based control and convergence.