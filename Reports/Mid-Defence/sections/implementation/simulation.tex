\subsection{Simulation Environment Implementation}

To enable systematic development and evaluation of antenna alignment
algorithms prior to hardware integration, a dedicated simulation environment
was implemented in Python. The simulation provides a controlled and repeatable
approximation of the wireless propagation, antenna directivity, and mechanical
actuation behavior of the proposed system.

The simulation environment is designed to emulate the interaction loop between
an adaptive controller and a directional antenna system. It exposes a
step-based interface compatible with reinforcement learning agents, while also
supporting conventional baseline alignment strategies. All algorithms
evaluated in this work operate on the same environment to ensure fairness and
comparability.


\subsubsection{Environment Structure and State Representation}

The simulation environment is implemented as a discrete-time Markov decision
process (MDP). At each time step, the environment maintains the antenna
orientation and generates a corresponding RSSI measurement.

The antenna orientation is parameterized by:
\begin{itemize}
    \item Azimuth angle $\theta \in [0^\circ, 360^\circ)$, discretized at a fixed angular resolution
    \item Elevation (tilt) angle $\phi \in [60^\circ, 120^\circ]$, discretized at $5^\circ$ resolution
\end{itemize}

In addition to absolute orientation, the reinforcement learning agent receives
a discretized RSSI trend indicator to capture short-term signal dynamics. The
RSSI trend is encoded into three states: increasing, stable, or decreasing.

The resulting state representation is given by:
\begin{equation}
s = (\theta_i, \phi_j, r_k)
\end{equation}
where $\theta_i$ and $\phi_j$ denote discrete azimuth and tilt indices, and
$r_k$ represents the RSSI trend state.

\subsubsection{RSSI Synthesis and Propagation Model}

The received signal strength is synthesized using a simplified yet physically
motivated propagation model. The RSSI at a given antenna orientation is
computed as a combination of path loss, antenna gain, and stochastic noise.

The path loss component is modeled using the log-distance path loss model:
\begin{equation}
PL(d) = PL(d_0) + 10n \log_{10}\left(\frac{d}{d_0}\right)
\end{equation}
where $d$ is the transmitter--receiver separation distance, $d_0$ is a
reference distance, and $n$ is the path loss exponent. In the simulation,
$d$ is fixed to represent a static transmitter location, allowing the effect
of antenna orientation to be isolated.

The received power is then expressed as:
\begin{equation}
RSSI = P_t + G_t(\theta, \phi) + G_r - PL(d) + \eta
\end{equation}
where $P_t$ is the transmit power, $G_t(\theta, \phi)$ is the directional gain
of the receiving antenna as a function of orientation, $G_r$ is the transmitter
antenna gain, and $\eta$ is additive noise.

\subsubsection{Noise Modeling and Measurement Variability}

To approximate real-world RSSI fluctuations observed in Wi-Fi systems,
additive noise is introduced into the synthesized RSSI measurements. The noise
term $\eta$ is modeled as zero-mean Gaussian noise:
\begin{equation}
\eta \sim \mathcal{N}(0, \sigma^2)
\end{equation}

Additionally, short-term averaging is applied to consecutive RSSI samples to
replicate packet-level RSSI smoothing performed on the embedded system. This
ensures that the simulated feedback closely resembles the characteristics of
RSSI data obtained from the MCU Wi-Fi interface.

\subsubsection{Action Space and Environment Dynamics}

The environment supports a discrete action space corresponding to incremental
antenna movements. Each action produces a bounded change in azimuth and/or
tilt angle, subject to mechanical constraints.

After an action is applied, the environment:
\begin{itemize}
    \item Updates the antenna orientation
    \item Applies a simulated settling delay
    \item Generates a new RSSI measurement based on the updated orientation
    \item Computes the reward as a function of RSSI improvement
\end{itemize}

This step-wise interaction model is shared across all evaluated algorithms,
ensuring identical dynamics for reinforcement learning and baseline methods.

\subsubsection{Algorithm Evaluation within the Simulation}

The simulation environment serves as a unified evaluation platform for all
alignment strategies considered in this work. The reinforcement learning agent
and baseline algorithms are executed over identical episodes, initial
conditions, and noise realizations.

Performance is evaluated using metrics such as:
\begin{itemize}
    \item Convergence time to optimal alignment
    \item Maximum achieved RSSI
    \item Number of antenna movements
    \item Stability under RSSI noise
\end{itemize}

This simulation-based evaluation enables controlled comparison and informed
algorithm selection prior to embedded deployment.
