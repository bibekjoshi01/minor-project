\subsection{Software and Firmware Design}

\subsubsection{Firmware Structure}

The firmware is organized as a single control loop with logically separated tasks, executed sequentially at each iteration. This design was chosen to match the constraints of a microcontroller-based system, where deterministic timing and simplicity are preferred over multi-threaded execution.

\textbf{Task Seperation}

Each iteration of the control loop performs the following tasks in order:

\begin{itemize}
\item \textbf{RSSI Sampling:} Acquire received signal strength measurements from the radio module
\item \textbf{State Encoding:} Convert the current antenna orientation and RSSI trend into a discrete reinforcement learning state
\item \textbf{Action Selection:} Select a control action (pan/tilt adjustment) using the trained Q-table
\item \textbf{Action Execution:} Translate the selected action into incremental pan and tilt updates
\item \textbf{Motor Control:} Generate step signals for the stepper motors to physically move the antenna
\end{itemize}

Although implemented in a single loop, this separation ensures that each functional responsibility remains clearly defined and independently testable.

\textbf{Main Loop Behavior}

The firmware operates in a cyclic manner, as illustrated below:

\begin{itemize}
\item Measure RSSI at the current antenna orientation
\item Compute the RSSI trend relative to previous measurements
\item Encode the current state and select an action from the Q-table
\item Apply the corresponding pan/tilt movement
\item Wait for mechanical settling and measurement stabilization
\item Repeat until a termination condition is met
\end{itemize}

This loop continues until either fixed number of alignment steps is reached, or no significant RSSI improvement is observed for several iterations.

\textbf{Timing Constraints}

Two dominant timing constraints govern the firmware behavior:

\begin{itemize}
\item \textbf{Motor Movement Delay:} Each discrete motor step requires a fixed settling time to ensure mechanical stability
\item \textbf{RSSI Measurement Delay:} After movement, a short delay is required before sampling RSSI to avoid transient effects
\end{itemize}

These delays are intentionally introduced to ensure reliable measurements and repeatable control behavior.

\textbf{Blocking vs Non-blocking Design Decisions}

The firmware uses a blocking execution model for motor movement and RSSI sampling. That is, the control loop waits for each operation to complete before proceeding to the next step. While non-blocking execution could improve responsiveness, it was not required for the alignment task and would increase implementation complexity. 

This design choice was made for the following reasons:

\begin{itemize}
\item Alignment speed is not real-time critical
\item Blocking simplifies synchronization between motor motion and RSSI measurement
\item It reduces firmware complexity compared to interrupt-driven or RTOS-based designs
\end{itemize}

\subsubsection{RSSI Processing and State Encoding}

The reinforcement learning agent operates on discrete states, whereas RSSI measurements and antenna angles are continuous. Therefore, signal processing and state encoding are required to bridge this gap.

\paragraph{RSSI Sampling and Averaging:}

RSSI measurements are inherently noisy due to environmental interference, multipath fading, and hardware variability. To mitigate short-term fluctuations, multiple RSSI samples are collected at each antenna orientation and averaged before further processing.

\begin{equation}
\overline{\text{RSSI}}_k = \frac{1}{N} \sum_{i=1}^{N} \text{RSSI}_{k,i}
\end{equation}

where:
\begin{itemize}
    \item $N$ is the number of RSSI samples collected per orientation,
    \item $\text{RSSI}_{k,i}$ is the $i$-th RSSI sample at iteration $k$,
    \item $\overline{\text{RSSI}}_k$ is the averaged RSSI value used by the control algorithm.
\end{itemize}

This averaging window improves measurement stability and ensures consistent reward estimation for the learning agent.

\paragraph{RSSI Trend Estimation:}

Rather than relying solely on absolute RSSI values, the system computes a short-term RSSI trend to capture whether recent actions are improving or degrading signal quality.

\begin{equation}
\Delta \text{RSSI}_k = \overline{\text{RSSI}}_k - \overline{\text{RSSI}}_{k-1}
\end{equation}

The RSSI trend is discretized into three qualitative categories using a threshold $\epsilon$:

\begin{equation}
\text{trend}_k =
\begin{cases}
+1, & \Delta \text{RSSI}_k > \epsilon \\
0,  & |\Delta \text{RSSI}_k| \le \epsilon \\
-1, & \Delta \text{RSSI}_k < -\epsilon
\end{cases}
\end{equation}

where $\epsilon$ is a small stability threshold used to suppress noise-induced fluctuations.

\paragraph{State Representation:}

The reinforcement learning state is defined using a compact discrete representation that captures both antenna orientation and recent signal behavior:

\begin{equation}
s_k = \left( i_{\text{pan}},\; i_{\text{tilt}},\; \text{trend}_k \right)
\end{equation}

where:
\begin{itemize}
    \item $i_{\text{pan}}$ is the discretized pan angle index,
    \item $i_{\text{tilt}}$ is the discretized tilt angle index,
    \item $\text{trend}_k \in \{-1, 0, +1\}$ represents RSSI decrease, stability, or increase.
\end{itemize}

\paragraph{Angle Discretization:}

Continuous pan and tilt angles are mapped to discrete indices using uniform angular resolution:

\begin{equation}
i_{\text{pan}} = \left\lfloor \frac{\text{pan}}{\Delta_{\text{pan}}} \right\rfloor,
\quad
i_{\text{tilt}} = \left\lfloor \frac{\text{tilt} - \text{tilt}_{\min}}{\Delta_{\text{tilt}}} \right\rfloor
\end{equation}

where $\Delta_{\text{pan}}$ and $\Delta_{\text{tilt}}$ denote the discretization step sizes for pan and tilt angles, respectively.

\paragraph{Q-table Dimensionality:}

This state encoding results in a Q-table with the following structure:

\begin{equation}
Q \in \mathbb{R}^{N_{\text{pan}} \times N_{\text{tilt}} \times 3 \times N_{\text{actions}}}
\end{equation}

where the third dimension corresponds to the three possible RSSI trend states.  
This formulation balances state expressiveness with tractable memory and training requirements.

\paragraph{State Encoding Pseudocode}

\begin{verbatim}
measure RSSI samples
avg_rssi = mean(samples)

delta = avg_rssi - prev_avg_rssi

if delta > eps:
    trend = +1
elif delta < -eps:
    trend = -1
else:
    trend = 0

pan_idx  = pan_angle // pan_step
tilt_idx = (tilt_angle - tilt_min) // tilt_step

state = (pan_idx, tilt_idx, trend)
\end{verbatim}